"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[959],{7945:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4/module4-vla","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview","source":"@site/docs/module4/index.md","sourceDirName":"module4","slug":"/module4/","permalink":"/AI_Book/docs/module4/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"id":"module4-vla","title":"Module 4: Vision-Language-Action (VLA)","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: NVIDIA Isaac Sim Basics","permalink":"/AI_Book/docs/module3/module3-chapter1"},"next":{"title":"Chapter 1: Vision-Language-Action Introduction","permalink":"/AI_Book/docs/module4/module4-chapter1"}}');var l=s(4848),t=s(8453);const o={id:"module4-vla",title:"Module 4: Vision-Language-Action (VLA)",sidebar_position:7},a="Module 4: Vision-Language-Action (VLA)",r={},c=[{value:"Overview",id:"overview",level:2},{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Components of a VLA System",id:"components-of-a-vla-system",level:2},{value:"1. Voice Input (ASR)",id:"1-voice-input-asr",level:3},{value:"2. LLM-Based Task Planning",id:"2-llm-based-task-planning",level:3},{value:"3. ROS 2 Action Execution",id:"3-ros-2-action-execution",level:3},{value:"End-to-End Pipeline Example",id:"end-to-end-pipeline-example",level:2},{value:"Conceptual Flow",id:"conceptual-flow",level:3},{value:"Implementation Sketch",id:"implementation-sketch",level:3},{value:"Assessment",id:"assessment",level:2},{value:"VLA Pipeline Project",id:"vla-pipeline-project",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,l.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," is a paradigm that enables robots to understand natural language commands and execute them in the physical world. VLA systems combine:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Vision"}),": Understanding the visual world through cameras"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Language"}),": Interpreting natural language commands"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Action"}),": Executing robot behaviors via ROS 2"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:'This module shows you how to build end-to-end VLA systems that translate "Clean the room" into a sequence of robot actions.'}),"\n",(0,l.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,l.jsx)(n.p,{children:"VLA bridges the gap between human communication and robot execution:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Human speaks"}),': "Pick up the red cup and place it on the table"']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"System transcribes"}),": Speech-to-text conversion"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"System understands"}),": LLM interprets the command and plans subtasks"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"System executes"}),": ROS 2 actions perform the planned behaviors"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"VLA makes robots accessible to non-technical users and enables complex, multi-step tasks."}),"\n",(0,l.jsx)(n.h2,{id:"components-of-a-vla-system",children:"Components of a VLA System"}),"\n",(0,l.jsx)(n.h3,{id:"1-voice-input-asr",children:"1. Voice Input (ASR)"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"})," converts speech to text:"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"OpenAI Whisper"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Open-source, high-accuracy ASR"}),"\n",(0,l.jsx)(n.li,{children:"Supports multiple languages"}),"\n",(0,l.jsx)(n.li,{children:"Can run locally or via API"}),"\n",(0,l.jsx)(n.li,{children:"Good balance of accuracy and latency"}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Alternative Options"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Google Speech-to-Text (cloud-based)"}),"\n",(0,l.jsx)(n.li,{children:"Azure Speech Services"}),"\n",(0,l.jsx)(n.li,{children:"Local models (Vosk, DeepSpeech)"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"2-llm-based-task-planning",children:"2. LLM-Based Task Planning"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," understand natural language and can generate structured plans:"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Task Planning Process"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"LLM receives natural language command"}),"\n",(0,l.jsx)(n.li,{children:"LLM breaks down command into subtasks"}),"\n",(0,l.jsx)(n.li,{children:"LLM maps subtasks to robot capabilities"}),"\n",(0,l.jsx)(n.li,{children:"LLM outputs structured plan (JSON or similar)"}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:'Input: "Clean the room"'}),"\n",(0,l.jsxs)(n.li,{children:["LLM Output:","\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",children:'{\n  "tasks": [\n    {"action": "navigate", "target": "living_room"},\n    {"action": "detect", "object": "trash"},\n    {"action": "pick", "object": "trash"},\n    {"action": "place", "object": "trash", "location": "trash_bin"}\n  ]\n}\n'})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"LLM Options"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"GPT-4 / Claude (cloud APIs, high capability)"}),"\n",(0,l.jsx)(n.li,{children:"LLaMA 2/3 (local deployment, privacy)"}),"\n",(0,l.jsx)(n.li,{children:"Specialized robotics LLMs (PaLM-E, RT-2)"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"3-ros-2-action-execution",children:"3. ROS 2 Action Execution"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"ROS 2 Actions"})," execute the planned behaviors:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Navigation actions"}),": Move to locations"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Manipulation actions"}),": Pick, place, grasp"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Perception actions"}),": Detect objects, scan environment"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Composite actions"}),": Sequences of simpler actions"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"end-to-end-pipeline-example",children:"End-to-End Pipeline Example"}),"\n",(0,l.jsx)(n.h3,{id:"conceptual-flow",children:"Conceptual Flow"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'User Voice Command\n    \u2193\n[ASR: Whisper]\n    \u2193\nText: "Clean the room"\n    \u2193\n[LLM: GPT-4]\n    \u2193\nStructured Plan (JSON)\n    \u2193\n[Task Executor]\n    \u2193\nROS 2 Actions\n    \u2193\nRobot Behavior\n'})}),"\n",(0,l.jsx)(n.h3,{id:"implementation-sketch",children:"Implementation Sketch"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"1. Speech Recognition"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import whisper\n\nmodel = whisper.load_model("base")\nresult = model.transcribe("user_audio.wav")\ncommand = result["text"]  # "Clean the room"\n'})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"2. Task Planning"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import openai\n\nresponse = openai.ChatCompletion.create(\n    model="gpt-4",\n    messages=[\n        {"role": "system", "content": "You are a robot task planner..."},\n        {"role": "user", "content": command}\n    ]\n)\nplan = parse_json(response.choices[0].message.content)\n'})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"3. Action Execution"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\n\n# Execute navigation action\nnav_client = ActionClient(node, NavigateToPose, 'navigate_to_pose')\ngoal = NavigateToPose.Goal()\ngoal.pose = plan['tasks'][0]['target']\nnav_client.send_goal_async(goal)\n\n# Execute manipulation actions\n# ... (similar pattern for pick, place, etc.)\n"})}),"\n",(0,l.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,l.jsx)(n.p,{children:"To demonstrate your understanding of VLA, complete the following:"}),"\n",(0,l.jsx)(n.h3,{id:"vla-pipeline-project",children:"VLA Pipeline Project"}),"\n",(0,l.jsx)(n.p,{children:"Build a minimal VLA system that:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Accepts voice input"})," (or text input for testing)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Transcribes to text"})," using Whisper or similar"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Generates a plan"})," using an LLM (GPT-4, Claude, or local model)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Executes a simple action"})," via ROS 2 (e.g., move forward, turn, or print a message)"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Success Criteria:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"System processes natural language commands"}),"\n",(0,l.jsx)(n.li,{children:"LLM generates reasonable task plans"}),"\n",(0,l.jsx)(n.li,{children:"At least one ROS 2 action executes based on the plan"}),"\n",(0,l.jsx)(n.li,{children:"Pipeline is documented with example inputs/outputs"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Stretch Goals:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Execute multiple actions in sequence"}),"\n",(0,l.jsx)(n.li,{children:"Handle error cases (unclear commands, failed actions)"}),"\n",(0,l.jsx)(n.li,{children:"Add feedback loop (robot reports status, system adjusts plan)"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"/AI_Book/docs/capstone_workflow/",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"/AI_Book/docs/hardware-lab-architecture/",children:"Hardware & Lab Architecture"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(6540);const l={},t=i.createContext(l);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);