"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[423],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>t});var s=i(6540);const l={},o=s.createContext(l);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),s.createElement(o.Provider,{value:n},e.children)}},8988:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/module4-chapter1","title":"Chapter 1: Vision-Language-Action Introduction","description":"Type: Theory-to-Practice","source":"@site/docs/module4/chapter-1-vla-introduction.md","sourceDirName":"module4","slug":"/module4/module4-chapter1","permalink":"/AI_Book/docs/module4/module4-chapter1","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"module4-chapter1","title":"Chapter 1: Vision-Language-Action Introduction","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/AI_Book/docs/module4/"},"next":{"title":"Hardware & Lab Architecture","permalink":"/AI_Book/docs/hardware-lab-architecture/"}}');var l=i(4848),o=i(8453);const r={id:"module4-chapter1",title:"Chapter 1: Vision-Language-Action Introduction",sidebar_position:1},t="Chapter 1: Vision-Language-Action Introduction",a={},c=[{value:"Chapter Overview",id:"chapter-overview",level:2},{value:"What is VLA?",id:"what-is-vla",level:2},{value:"VLA Workflow",id:"vla-workflow",level:3},{value:"Components Overview",id:"components-overview",level:2},{value:"1. Speech Recognition (ASR)",id:"1-speech-recognition-asr",level:3},{value:"2. Large Language Models (LLMs)",id:"2-large-language-models-llms",level:3},{value:"3. ROS 2 Action Execution",id:"3-ros-2-action-execution",level:3},{value:"Building a Minimal VLA System",id:"building-a-minimal-vla-system",level:2},{value:"Step 1: Speech Recognition",id:"step-1-speech-recognition",level:3},{value:"Step 2: Task Planning",id:"step-2-task-planning",level:3},{value:"Step 3: Action Execution",id:"step-3-action-execution",level:3},{value:"Chapter Projects",id:"chapter-projects",level:2},{value:"Project 1: Speech Recognition",id:"project-1-speech-recognition",level:3},{value:"Project 2: Task Planning",id:"project-2-task-planning",level:3},{value:"Project 3: Minimal VLA Pipeline",id:"project-3-minimal-vla-pipeline",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"chapter-1-vision-language-action-introduction",children:"Chapter 1: Vision-Language-Action Introduction"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Type"}),": Theory-to-Practice",(0,l.jsx)(n.br,{}),"\n",(0,l.jsx)(n.strong,{children:"Lessons"}),": 6",(0,l.jsx)(n.br,{}),"\n",(0,l.jsx)(n.strong,{children:"Duration"}),": 10-12 hours"]}),"\n",(0,l.jsx)(n.h2,{id:"chapter-overview",children:"Chapter Overview"}),"\n",(0,l.jsx)(n.p,{children:"This chapter introduces Vision-Language-Action (VLA), the paradigm that enables robots to understand natural language commands and execute them in the physical world. You'll learn the components of VLA systems and how they integrate."}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"By the end of this chapter, you will"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Understand what VLA is and why it matters"}),"\n",(0,l.jsx)(n.li,{children:"Know the components: Vision, Language, Action"}),"\n",(0,l.jsx)(n.li,{children:"Understand speech recognition (ASR)"}),"\n",(0,l.jsx)(n.li,{children:"Understand LLM-based task planning"}),"\n",(0,l.jsx)(n.li,{children:"See how ROS 2 actions execute plans"}),"\n",(0,l.jsx)(n.li,{children:"Build a minimal VLA pipeline"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," combines:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Vision"}),": Understanding the visual world"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Language"}),": Interpreting natural language"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Action"}),": Executing robot behaviors"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"vla-workflow",children:"VLA Workflow"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"User Voice Command\n    \u2193\n[ASR: Speech \u2192 Text]\n    \u2193\n[LLM: Text \u2192 Task Plan]\n    \u2193\n[Action Executor: Plan \u2192 ROS 2 Actions]\n    \u2193\nRobot Behavior\n"})}),"\n",(0,l.jsx)(n.h2,{id:"components-overview",children:"Components Overview"}),"\n",(0,l.jsx)(n.h3,{id:"1-speech-recognition-asr",children:"1. Speech Recognition (ASR)"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"OpenAI Whisper"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Open-source, high-accuracy ASR"}),"\n",(0,l.jsx)(n.li,{children:"Supports multiple languages"}),"\n",(0,l.jsx)(n.li,{children:"Can run locally or via API"}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Alternatives"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Google Speech-to-Text"}),"\n",(0,l.jsx)(n.li,{children:"Azure Speech Services"}),"\n",(0,l.jsx)(n.li,{children:"Local models (Vosk, DeepSpeech)"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"2-large-language-models-llms",children:"2. Large Language Models (LLMs)"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"For Task Planning"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"GPT-4 / Claude: High capability, cloud APIs"}),"\n",(0,l.jsx)(n.li,{children:"LLaMA 2/3: Local deployment, privacy"}),"\n",(0,l.jsx)(n.li,{children:"Specialized: PaLM-E, RT-2"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"3-ros-2-action-execution",children:"3. ROS 2 Action Execution"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Action Types"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Navigation actions"}),"\n",(0,l.jsx)(n.li,{children:"Manipulation actions"}),"\n",(0,l.jsx)(n.li,{children:"Perception actions"}),"\n",(0,l.jsx)(n.li,{children:"Composite actions"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"building-a-minimal-vla-system",children:"Building a Minimal VLA System"}),"\n",(0,l.jsx)(n.h3,{id:"step-1-speech-recognition",children:"Step 1: Speech Recognition"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import whisper\n\nmodel = whisper.load_model("base")\nresult = model.transcribe("user_audio.wav")\ncommand = result["text"]  # "Clean the room"\n'})}),"\n",(0,l.jsx)(n.h3,{id:"step-2-task-planning",children:"Step 2: Task Planning"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import openai\n\nresponse = openai.ChatCompletion.create(\n    model="gpt-4",\n    messages=[\n        {"role": "system", "content": "You are a robot task planner..."},\n        {"role": "user", "content": command}\n    ]\n)\nplan = parse_json(response.choices[0].message.content)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"step-3-action-execution",children:"Step 3: Action Execution"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\n\n# Execute navigation action\nnav_client = ActionClient(node, NavigateToPose, 'navigate_to_pose')\ngoal = NavigateToPose.Goal()\ngoal.pose = plan['tasks'][0]['target']\nnav_client.send_goal_async(goal)\n"})}),"\n",(0,l.jsx)(n.h2,{id:"chapter-projects",children:"Chapter Projects"}),"\n",(0,l.jsx)(n.h3,{id:"project-1-speech-recognition",children:"Project 1: Speech Recognition"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Set up Whisper"}),"\n",(0,l.jsx)(n.li,{children:"Transcribe voice commands"}),"\n",(0,l.jsx)(n.li,{children:"Handle different languages"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"project-2-task-planning",children:"Project 2: Task Planning"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Integrate LLM (GPT-4 or local)"}),"\n",(0,l.jsx)(n.li,{children:"Generate structured plans"}),"\n",(0,l.jsx)(n.li,{children:"Parse plan JSON"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"project-3-minimal-vla-pipeline",children:"Project 3: Minimal VLA Pipeline"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Combine ASR + LLM + Actions"}),"\n",(0,l.jsx)(n.li,{children:"Execute simple commands"}),"\n",(0,l.jsx)(n.li,{children:"Handle errors gracefully"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"VLA"})," enables natural language robot control"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Three components"}),": Vision, Language, Action"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"ASR"})," converts speech to text"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"LLMs"})," generate task plans from text"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"ROS 2 Actions"})," execute plans on robots"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"/AI_Book/docs/module4/module4-chapter2",children:"Chapter 2: Advanced VLA Systems"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"/AI_Book/docs/module4/",children:"Return to Module 4 Overview"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}}}]);